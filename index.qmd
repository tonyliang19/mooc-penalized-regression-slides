---
title: "Penalized Regression"
subtitle: "Understanding LASSO and Ridge Regression"
author: "Tony Liang"
filters:
    - webr
webr:
    packages:
        - dplyr
        - ggplot2
        - glmnet
        - mvtnorm
        - tidyr
    show-starup-message: false
---

## Welcome

This presentation covers:

- Quick review of Ordinary Least Squares (OLS)
- Introduction to penalized regression methods
- Ridge Regression (L2 penalty)
- LASSO Regression (L1 penalty)
- Practical examples with interactive R code

## What You'll Learn

By the end of this session:

- Understand limitations of OLS regression
- Learn how regularization helps with overfitting
- Distinguish between Ridge and LASSO approaches
- Apply these methods to real data

---

## Part 1: OLS Review {.center2}

## Ordinary Least Squares (OLS) Basics

**The Classic Linear Model:**

$$y = X\beta + \epsilon$$

- Goal: Minimize the sum of squared residuals
- $$\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n}(y_i - x_i^T\beta)^2$$
- Closed form solution: $\hat{\beta} = (X^TX)^{-1}X^Ty$

## OLS: Simple Example

Let's see OLS in action:

```{webr-r}
# Generate simple data
set.seed(123)
n <- 50
x <- rnorm(n)
y <- 2 + 3*x + rnorm(n, sd=0.5)

# Fit OLS model
model <- lm(y ~ x)
summary(model)$coefficients
```

## OLS: Visualization

```{webr-r}
# Plot the data and fitted line
plot(x, y, pch=19, col='steelblue',
     main='OLS Regression', xlab='X', ylab='Y')
abline(model, col='red', lwd=2)
legend('topleft', 'Fitted line', col='red', lwd=2)
```

## OLS Assumptions

Key assumptions for OLS:

- **Linearity**: Relationship between X and Y is linear
- **Independence**: Observations are independent
- **Homoscedasticity**: Constant variance of errors
- **Normality**: Errors are normally distributed

## Problems with OLS

OLS can struggle when:

- **High dimensionality**: More predictors than observations (p > n)
- **Multicollinearity**: Predictors are highly correlated
- **Overfitting**: Model fits noise in training data
- **Interpretation**: Too many variables make models hard to understand



## Part 2: Why Penalized Regression? {.center2}

## The Overfitting Problem

As model complexity increases:

- **Training error** decreases
- **Test error** may increase (overfitting)
- Model captures noise rather than signal

**Solution**: Add a penalty to control model complexity

## Bias-Variance Tradeoff

The fundamental tradeoff:

- **Low Complexity**: High bias, low variance (underfitting)
- **High Complexity**: Low bias, high variance (overfitting)
- **Optimal**: Balance between the two

Penalized regression helps find this balance


```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4

# call library here
library(ggplot2)

# Parameters
mu = 1; n = 5; sig = 1

secondary = "#e98a15"
primary = "#2c365e"
tertiary = "#0a8754"
fourth_color = "#a8201a"

## for simplicity
purple = primary
blue = primary
orange = secondary
green = tertiary
red = fourth_color

cols = c(blue, red, green, orange)
par(mfrow = c(2, 2), bty = "n", ann = FALSE, xaxt = "n", yaxt = "n", 
    family = "serif", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))
library(mvtnorm)
mv <- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)
va <- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)

for (i in 1:4) {
  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, 
       col = blue, ann = FALSE, pty = "s")
  points(0, 0, pch = 19, cex = 30, col = "white")
  points(0, 0, pch = 19, cex = 18, col = green)
  points(0, 0, pch = 19, cex = 6, col = orange)
  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)
  switch(i,
    "1" = {
      mtext("low variance", 3, cex = 2)
      mtext("low bias", 2, cex = 2)
    },
    "2" = mtext("high variance", 3, cex = 2),
    "3" = mtext("high bias", 2, cex = 2)
  )
}
```


## General Penalized Regression Form

$$\hat{\beta} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - x_i^T\beta)^2 + \lambda \, \lVert \beta \rVert_{x} \right\}$$

- First term: Measures fit to data (like OLS)
- Second term: Penalizes model complexity with different norms
    - x is the different norm to use
- $\lambda \geq 0$: Tuning parameter controlling penalty strength

## Different norms


```{r}
#| echo: false
library(mvtnorm)
library(dplyr)
normBall <- function(q = 1, len = 1000) {
  tg <- seq(0, 2 * pi, length = len)
  out <- data.frame(x = cos(tg)) |>
    mutate(b = (1 - abs(x)^q)^(1 / q), bm = -b) |>
    tidyr::gather(key = "lab", value = "y", -x)
  out$lab <- paste0('"||" * beta * "||"', "[", signif(q, 2), "]")
  return(out)
}

ellipseData <- function(n = 100, xlim = c(-2, 3), ylim = c(-2, 3),
                        mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {
  df <- expand.grid(
    x = seq(xlim[1], xlim[2], length.out = n),
    y = seq(ylim[1], ylim[2], length.out = n)
  )
  df$z <- dmvnorm(df, mean, Sigma)
  df
}

lballmax <- function(ed, q = 1, tol = 1e-6) {
  ed <- filter(ed, x > 0, y > 0)
  for (i in 1:20) {
    ff <- abs((ed$x^q + ed$y^q)^(1 / q) - 1) < tol
    if (sum(ff) > 0) break
    tol <- 2 * tol
  }
  best <- ed[ff, ]
  best[which.max(best$z), ]
}

nbs <- list()
nbs[[1]] <- normBall(0, 1)
qs <- c(.5, .75, 1, 1.5, 2)
for (ii in 2:6) nbs[[ii]] <- normBall(qs[ii - 1])
nbs <- bind_rows(nbs)
nbs$lab <- factor(nbs$lab, levels = unique(nbs$lab))
seg <- data.frame(
  lab = levels(nbs$lab)[1],
  x0 = c(-1, 0), x1 = c(1, 0), y0 = c(0, -1), y1 = c(0, 1)
)
levels(seg$lab) <- levels(nbs$lab)
ggplot(nbs, aes(x, y)) +
  geom_path(size = 1.2) +
  facet_wrap(~lab, labeller = label_parsed) +
  geom_segment(data = seg, aes(x = x0, xend = x1, y = y0, yend = y1), size = 1.2) +
  theme_bw(base_family = "", base_size = 24) +
  coord_equal() +
  scale_x_continuous(breaks = c(-1, 0, 1)) +
  scale_y_continuous(breaks = c(-1, 0, 1)) +
  geom_vline(xintercept = 0, size = .5) +
  geom_hline(yintercept = 0, size = .5) +
  xlab(bquote(beta[1])) +
  ylab(bquote(beta[2]))
```




## Part 3: Ridge Regression {.center2}

## Ridge Regression (L2 Penalty)

**Definition:**

$$\hat{\beta}^{\text{Ridge}} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 \right\}$$

- Adds squared magnitude of coefficients as penalty
- Also called "L2 regularization"
- Has closed form: $\hat{\beta}^{\text{Ridge}} = (X^TX + \lambda I)^{-1}X^Ty$

## Ridge: Key Properties

**Characteristics:**

- **Shrinks** coefficients toward zero (but not exactly to zero)
- **Handles multicollinearity** effectively
- **Works when p > n** (more predictors than observations)
- All predictors remain in the model

**When to use**: Many correlated predictors, want to keep all variables

## Ridge: Effect of Lambda

```{webr-r}
# Generate data with correlated predictors
set.seed(42)
n <- 100
p <- 5
X <- matrix(rnorm(n*p), n, p)
# Make some predictors correlated
X[,2] <- X[,1] + rnorm(n, sd=0.1)
beta_true <- c(3, 1.5, 0, 0, 2)
y <- X %*% beta_true + rnorm(n)

# Show correlation
cor(X)[1:2, 1:2]
```

## Ridge: Fitting the Model

```{webr-r}
library(glmnet)

# Fit ridge regression (alpha=0 for ridge)
ridge_model <- glmnet(X, y, alpha=0)

# Plot coefficient paths
plot(ridge_model, xvar="lambda", label=TRUE,
     main="Ridge Coefficient Paths")
```

## Ridge: Choosing Lambda

**Cross-Validation** is the standard approach:

```{webr-r}
# Cross-validation to find optimal lambda
cv_ridge <- cv.glmnet(X, y, alpha=0, nfolds=5)
plot(cv_ridge)
```

## Ridge: Best Model

```{webr-r}
# Best lambda and coefficients
best_lambda <- cv_ridge$lambda.min
cat("Optimal lambda:", round(best_lambda, 4), "\n")

# Coefficients at optimal lambda
coef(cv_ridge, s="lambda.min")
```



## Part 4: LASSO Regression {.center2}

## LASSO Regression (L1 Penalty)

**Definition:**

$$\hat{\beta}^{\text{LASSO}} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^{p}|\beta_j| \right\}$$

- "Least Absolute Shrinkage and Selection Operator"
- Uses absolute value of coefficients as penalty
- Also called "L1 regularization"

## LASSO: Key Properties

**Characteristics:**

- **Shrinks** some coefficients **exactly to zero**
- **Performs variable selection** automatically
- **Creates sparse models** (fewer non-zero coefficients)
- No closed form solution (requires optimization)

**When to use**: Many predictors, want automatic feature selection

## LASSO: The Magic of L1

Why does L1 penalty produce zeros?

- **Geometric intuition**: L1 constraint region has corners
- Optimization often hits corners where some $\beta_j = 0$
- Ridge's L2 constraint is a circle (no corners)


This is why LASSO does variable selection!

## LASSO: Fitting the Model

```{webr-r}
# Fit LASSO regression (alpha=1 for LASSO)
lasso_model <- glmnet(X, y, alpha=1)

# Plot coefficient paths
plot(lasso_model, xvar="lambda", label=TRUE,
     main="LASSO Coefficient Paths")
```

## LASSO: Variable Selection

```{webr-r}
# Cross-validation for LASSO
cv_lasso <- cv.glmnet(X, y, alpha=1, nfolds=5)
plot(cv_lasso)
```

## LASSO: Selected Variables

```{webr-r}
# Best lambda
best_lambda_lasso <- cv_lasso$lambda.min
cat("Optimal lambda:", round(best_lambda_lasso, 4), "\n\n")

# Coefficients - note some are exactly zero!
coef(cv_lasso, s="lambda.min")
```


## Part 5: Comparison {.center2}

## Ridge vs LASSO: Side by Side

| Aspect | Ridge (L2) | LASSO (L1) |
|--------|-----------|------------|
| Penalty | $\sum \beta_j^2$ | $\sum \|\beta_j\|$ |
| Coefficients | Shrinks toward 0 | Can be exactly 0 |
| Variable Selection | No | Yes |
| Multicollinearity | Handles well | Picks one arbitrarily |
| Use Case | Keep all predictors | Feature selection |

## When to Use Which?

**Use Ridge when:**

- All predictors might be relevant
- Predictors are highly correlated
- Want to shrink coefficients but keep all

**Use LASSO when:**

- Many predictors are likely irrelevant
- Want a simpler, interpretable model
- Need automatic feature selection


## Part 6: Practical Considerations

## Standardization is Critical

**Always standardize predictors** before applying penalized regression:

- Different scales $\rightarrow$ different penalties
- `glmnet` standardizes by default
- Coefficients returned on original scale

```{webr-r}
# Manual standardization example
X_scaled <- scale(X)
head(X_scaled[, 1:3], 3)
```

## Choosing the Tuning Parameter

**Cross-Validation** is the gold standard:

- Split data into K folds (typically 5 or 10)
- For each $\lambda$, train on K-1 folds, validate on 1
- Choose $\lambda$ with lowest CV error

**Two common choices:**

- `lambda.min`: Minimum CV error
- `lambda.1se`: Largest $\lambda$ within 1 SE of minimum (more regularization)

## Interpreting Results

After fitting penalized regression:

- **Non-zero coefficients**: Important predictors
- **Zero coefficients** (LASSO): Excluded from model
- **Magnitude**: Strength of relationship (on standardized scale)
- **Sign**: Direction of relationship

## Common Pitfalls

**Watch out for:**

- **Not standardizing**: Unfair penalties on different scales
- **Data leakage**: Standardizing before splitting train/test
- **Ignoring λ.1se**: Sometimes simpler model is better
- **Not validating**: Always test on held-out data


## Part 7: Hands-On Exercise {.center2}

## Your Turn: Complete Example

Let's build a model from scratch:

```{webr-r}
# Generate data
set.seed(100)
n <- 200
p <- 20
X <- matrix(rnorm(n*p), n, p)

# Only 5 predictors truly matter
true_coef <- c(3, -2, 1.5, -1, 2, rep(0, 15))
y <- X %*% true_coef + rnorm(n, sd=2)

cat("Data generated: n =", n, ", p =", p, "\n")
cat("True non-zero coefficients:", sum(true_coef != 0), "\n")
```

## Exercise: Compare Models

```{webr-r}
# Compare Ridge vs LASSO
cv_ridge_ex <- cv.glmnet(X, y, alpha=0, nfolds=10)
cv_lasso_ex <- cv.glmnet(X, y, alpha=1, nfolds=10)

cat("Ridge CV error:", round(min(cv_ridge_ex$cvm), 3), "\n")
cat("LASSO CV error:", round(min(cv_lasso_ex$cvm), 3), "\n")
```


## Summary

## Key Takeaways

**OLS Limitations:**

- Struggles with high dimensions and multicollinearity
- Prone to overfitting with many predictors

**Ridge Regression:**

- Shrinks coefficients with L2 penalty
- Keeps all predictors in model
- Great for correlated predictors

## Key Takeaways (continued)

**LASSO Regression:**

- Shrinks with L1 penalty, produces exact zeros
- Automatic variable selection
- Creates sparse, interpretable models

**Practical:**

- Always standardize predictors
- Use cross-validation to choose λ
- Consider Elastic Net for best of both worlds


## Resources

**R Packages:**

- `glmnet`: Fast implementation of elastic net
- `caret`: ML framework with penalized regression
- `tidymodels`: Modern framework for modeling

**Books:**

- "Statistical Learning with Sparsity" (Hastie, Tibshirani, Wainwright)
- "An Introduction to Statistical Learning" (James et al.)
- "Elements of Statistical Learning" (Hastie, Tibshirani, Friedman)

## Questions?

Thank you for your attention!

**Feel free to:**

- Experiment with the interactive code
- Try different values of α and λ
- Apply these methods to your own data

